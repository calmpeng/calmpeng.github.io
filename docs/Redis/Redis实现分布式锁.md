### 如何使用Redis实现分布式锁

#### 背景

在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是**保存在一个共享存储系统中（锁不能是某个客户端本地的锁）**的，可以被多个客户端共享访问和获取。

分布式锁是由共享存储系统维护的变量，多个客户端可以向共享存储系统发送命令进行加锁或释放锁操作。Redis作为一个共享存储系统，可以用来实现分布式锁。

#### 单机上的锁和分布式锁的联系与区别

在单机上运行的多线程程序来说，锁本身可以用一个变量表示。

* 变量值为0时，表示没有线程获取锁；
* 变量值为1时，表示已经有线程获取到锁了

实际上，一个线程调用加锁操作，其实就是检查锁变量值是否为0。如果是0，就把锁的变量值设置为1，表示获取到锁，如果不是0，就返回错误信息，表示加锁失败，已经有别的线程获取到锁了。而一个线程调用释放锁操作，其实就是将锁变量的值置为0，以便其它线程可以来获取锁。

分布式锁同样可以**用一个变量来实现**。客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：**加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为0，表明客户端不再持有锁**。

在分布式场景下，**锁变量需要由一个共享存储系统来维护**，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，**加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值**。

**实现分布式锁的两个要求**

* 分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要**保证这些锁操作的原子性**

* 共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证**共享存储系统的可靠性，进而保证锁的可靠性**

#### 基于单个Redis节点实现分布式锁

赋予锁变量一个变量名，把这个变量名作为键值对的键，而锁变量的值，则是键值对的值，这样一来，Redis就能保存锁变量了，客户端也就可以通过Redis的命令操作来实现锁操作

**加锁** 

![image-20220331231248079](https://raw.githubusercontent.com/calmpeng/photo/main/img/image-20220331231248079.png)

>  客户端A和C同时请求加锁。因为**Redis使用单线程处理请求**，所以，即使客户端A和C同时把加锁请求发给了Redis，**Redis也会串行处理它们的请求**。

**解锁** 客户端A执行释放锁操作后，Redis将lock_key的值置为0，表明已经没有客户端持有锁了。

![image-20220331231447442](https://raw.githubusercontent.com/calmpeng/photo/main/img/image-20220331231447442.png)

加锁包含了三个操作（读取锁变量、判断锁变量值以及把锁变量值设置为1），而这三个操作在执行时需要保证原子性。那**怎么保证原子性**呢？

想保证操作的原子性，有两种通用的方法，分别是使用Redis的单命令操作和使用Lua脚本

`SETNX`命令，它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。

对于释放锁操作来说，我们可以在执行完业务逻辑后，使用`DEL`命令删除锁变量

总结来说，我们就可以用`SETNX`和`DEL`命令组合来实现加锁和释放锁操作。

```lua
// 加锁
SETNX lock_key 1
// 业务逻辑
DO THINGS
// 释放锁
DEL lock_key
```

两个潜在的风险

* 第一个风险是，假如某个客户端在执行了SETNX命令、加锁之后，紧接着却在操作共享数据时发生了异常，结果一直没有执行最后的DEL命令释放锁。 一个有效的解决方法是，**给锁变量设置一个过期时间**

  SET命令在执行时还可以带上EX或PX选项，用来设置键值对的过期时间

  ```shell
  SET key value [EX seconds | PX milliseconds]  [NX]
  // 加锁, unique_value作为客户端唯一性的标识
  SET lock_key unique_value NX PX 10000
  ```

* 第二风险，客户端A加了锁，被其他客户端B，释放了。需要**区分来自不同客户端的锁操作**所以，我们在加锁操作时，**可以让每个客户端给锁变量设置一个唯一值**，这里的唯一值就可以用来标识当前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一标识相等，只有在相等的情况下，才能释放锁。这样一来，就不会出现误释放锁的问题了。

  每个客户端都使用了一个唯一标识，所以在释放锁操作时，我们需要判断锁变量的值，是否等于执行释放锁操作的客户端的唯一标识

  ```lua
  //释放锁 比较unique_value是否相等，避免误释放
  if redis.call("get",KEYS[1]) == ARGV[1] then
      return redis.call("del",KEYS[1])
  else
      return 0
  end
  ```

  这是使用Lua脚本（unlock.script）实现的释放锁操作的伪代码，其中，KEYS[1]表示lock_key，ARGV[1]是当前客户端的唯一标识，这两个值都是我们在执行Lua脚本时作为参数传入的

  最后，我们执行 `edis-cli  --eval  unlock.script lock_key , unique_value `	在释放锁操作中，我们使用了Lua脚本，这是因为，释放锁操作的逻辑也包含了读取锁变量、判断值、删除锁变量的多个操作，而Redis在执行Lua脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。

#### 基于多个Redis节点实现高可靠的分布式锁

##### 分布式锁算法Redlock

让客户端和**多个独立的Redis实例依次请求加锁**，如果客户端能够和**半数以上的实例成功地完成加锁操作**，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个Redis实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失

**第一步是，客户端获取当前时间。**

**第二步是，客户端按顺序依次向N个Redis实例执行加锁操作。**

加锁操作和在单实例上执行的加锁操作一样，使用SET命令，带上NX，EX/PX选项，以及带上客户端的唯一标识。加锁操作的超时时间需要远远地小于锁的有效时间

**第三步是，一旦客户端完成了和所有Redis实例的加锁操作，客户端就要计算整个加锁过程的总耗时。**

客户端只有在满足下面的这两个条件时，才能认为是加锁成功。

- 条件一：客户端从**超过半数**（大于等于 N/2+1）的Redis实例上成功获取到了锁；
- 条件二：客户端获取锁的**总耗时没有超过锁的有效时间**。

在满足了这两个条件后，我们需要**重新计算这把锁的有效时间**，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间**已经来不及完成共享数据的操作了，我们可以释放锁**，以免出现还没完成数据操作，锁就过期了的情况。

如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，**客户端向所有Redis节点发起释放锁的操作**。

在Redlock算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的Lua脚本就可以了。这样一来，只要N个Redis实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。

> 基于 Redis 使用分布锁的注意点
>
> * 使用 SET $lock_key $unique_val EX $second NX 命令保证加锁原子性，并为锁设置过期时间
> * 锁的过期时间要提前评估好，要大于操作共享资源的时间
> * 每个线程加锁时设置随机值，释放锁时判断是否和加锁设置的值一致，防止自己的锁被别人释放
> * 释放锁时使用 Lua 脚本，保证操作的原子性
> * 基于多个节点的 Redlock，加锁时超过半数节点操作成功，并且获取锁的耗时没有超过锁的有效时间才算加锁成功
> * Redlock 释放锁时，要对所有节点释放（即使某个节点加锁失败了），因为加锁时可能发生服务端加锁成功，由于网络问题，给客户端回复网络包失败的情况，所以需要把所有节点可能存的锁都释放掉
> * 使用 Redlock 时要避免机器时钟发生跳跃，需要运维来保证，对运维有一定要求，否则可能会导致 Redlock 失效。例如共 3 个节点，线程 A 操作 2 个节点加锁成功，但其中 1 个节点机器时钟发生跳跃，锁提前过期，线程 B 正好在另外 2 个节点也加锁成功，此时 Redlock 相当于失效了 [Redis 作者和分布式系统专家争论](http://zhangtielei.com/posts/blog-redlock-reasoning.html)
> * 如果为了效率，使用基于单个 Redis 节点的分布式锁即可，此方案缺点是允许锁偶尔失效，优点是简单效率高
> * 如果是为了正确性，业务对于结果要求非常严格，建议使用 Redlock，但缺点是使用比较重，部署成本高 



---

参考[redis](http://118.25.23.115/redis/30-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%EF%BC%9F.html)

