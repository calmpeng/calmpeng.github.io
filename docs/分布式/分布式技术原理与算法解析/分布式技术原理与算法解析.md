### 分布式技术原理与算法解析

#### 开篇词

> 既然指令可以重用，代码可以重用，技术、业务、数据等都可以重用，为什么知识体系不可以呢？学好分布式通识课，掌握了分布式的核心技术、体系，你就会发现很多新技术、新框架、新组件只不过是‘新瓶装旧酒’，将分布式核心技术进行了再包装、再组合，至多也就是做了一点延伸而已。

按照业务的架构层次栈，我自底向上按照资源、通信、数据与计算的维度，梳理出了 4 个技术层次：**分布式资源池化、分布式通信、分布式数据存储与管理、分布式计算**。这样的划分符合业务架构设计的一般规律，即“在一定资源上，进行一定通信，通过一定计算，完成一定数据的加工和处理，从而对外提供特定的服务”。

![知识体系图](https://static001.geekbang.org/resource/image/62/e9/62b5acf80f49fe8d1a8b229df36f00e9.jpg)

在分布式环境下，无论是资源、通信、数据还是计算，都需要去解决**协同、调度、追踪高可用，还有部署**的问题。因此，我从横向的技术层次中，提炼出**分布式协同、分布式调度、分布式追踪与高可用、分布式部署** 4 个纵向技术线。

#### 起源

单机模式到数据并行（也叫作数据分布式）模式，再到任务并行（也叫作任务分布式）模式

单机模式指的是，**所有业务和数据均部署到同一台机器上**。这种模式的好处是功能、代码和数据集中，便于维护、管理和执行，但**计算效率是瓶颈**。也就是说单机模式性能受限，也存在**单点失效**的问题。

数据并行（也叫作数据分布式）模式指的是，对**数据进行拆分**，利用多台计算机并行执行多个相同任务，**通过在相同的时间内完成多个相同任务**，从而缩短所有任务的总体执行时间，**但对提升单个任务的执行性能及降低时延无效**。

任务并行（也叫作任务分布式）模式指的是，**单任务按照执行流程，拆分成多个子任务，多个子任务分别并行执行**，只要一个复杂任务中的任意子任务的执行时间变短了，那么这个业务的整体执行时间也就变短了。该模式在提高性能、扩展性、可维护性等的同时，也带来了设计上的复杂性问题，比如复杂任务的拆分。

到底是采用数据并行还是任务并行呢？一个简单的原则就是：**任务执行时间短，数据规模大、类型相同且无依赖，则可采用数据并行；如果任务复杂、执行时间长，且任务可拆分为多个子任务，则考虑任务并行。**在实际业务中，通常是这两种模式并用。

> 传统的并行计算是时空上复用多处理器 
>
> 分布式是空间上给任务分配多处理器

#### 指标信息

分布式的目的是用更多的机器，处理更多的数据和更复杂的任务。**性能、资源、可用性和可扩展性**是分布式系统的重要指标。

##### 性能（Performance）

吞吐量（Throughput）系统在一定时间内可以处理的任务数。

* QPS，即查询数每秒，用于衡量一个系统每秒处理的查询数。这个指标通常用于读操作，越高说明对读操作的支持越好。
* TPS，即事务数每秒，用于衡量一个系统每秒处理的事务数。这个指标通常对应于写操作，越高说明对写操作的支持越好。
* BPS，即比特数每秒，用于衡量一个系统每秒处理的数据量。对于一些网络系统、数据管理系统，我们不能简单地按照请求数或事务数来衡量其性能。

响应时间（Response Time）系统响应一个请求或输入需要花费的时间。响应时间直接影响到用户体验，对于时延敏感的业务非常重要。

和完成时间（Turnaround Time）系统真正完成一个请求或处理需要花费的时间。任务并行（也叫作任务分布式）模式出现的其中一个目的，就是缩短整个任务的完成时间。

##### 资源占用（Resource Usage）

资源占用指的是，一个系统**提供正常能力需要占用的硬件资源**，比如 CPU、内存、硬盘等。

一个系统在没有任何负载时的资源占用，叫做**空载资源占用**，体现了这个系统自身的资源占用情况。

一个系统满额负载时的资源占用，叫做**满载资源占用**，体现了这个系统全力运行时占用资源的情况，也体现了系统的处理能力。

##### 可用性（Availability）

可用性，通常指的是系统在**面对各种异常时可以正确提供服务的能力。**可用性是分布式系统的一项重要指标，衡量了系统的鲁棒性，是系统容错能力的体现。

系统的可用性可以用系统停止服务的时间与总的时间之比衡量。系统的可用性还可以用某功能的失败次数与总的请求次数之比来衡量。

**可靠性通常用来表示一个系统完全不出故障的概率**，更多地用在硬件领域。而**可用性则更多的是指在允许部分组件失效的情况下，一个系统对外仍能正常提供服务的概率**。

##### 可扩展性（Scalability）

可扩展性，指的是分布式系统通过扩展集群机器规模提高系统性能 (吞吐量、响应时间、 完成时间)、存储容量、计算能力的特性，是分布式系统的特有性质。

> 软件测试是这么写的 可伸缩性和可拓展性 
>
> 可伸缩性翻译自 Scalability，指的是通过简单地增加硬件配置而使服务处理能力呈线性增长的能力。最简单直观的例子，就是通过在应用服务器集群中增加更多的节点，来提高整个集群的处理能力。 
>
> 可扩展性翻译自 Extensibility，指的是网站的架构设计能够快速适应需求的变化，当需要增加新的功能实现时，对原有架构不需要做修改或者做很少的修改就能够快速满足新的业务需求。

#### 分布式互斥

在分布式系统里，这种**排他性的资源访问方式，叫作分布式互斥**（Distributed Mutual Exclusion），而这种被互斥访问的共享资源就叫作临界资源（Critical Resource）。

##### 霸道总裁：集中式算法

我们**引入一个协调者程序**，得到一个分布式互斥算法。每个程序在需要访问临界资源时，**先给协调者发送一个请求**。如果当前没有程序使用这个资源，协调者直接授权请求程序访问；否则，**按照先来后到的顺序为请求程序“排一个号”**。如果有程序使用完资源，则通知协调者，协调者从“排号”的队列里取出排在最前面的请求，并给它发送授权消息。拿到授权消息的程序，可以直接去访问临界资源。

集中式算法的优点在于**直观、简单、信息交互量少、易于实现**，并且所有程序只需和协调者通信，程序之间无需通信。但是，这个算法的问题也出在了协调者身上。

* 一方面，协调者会成为系统的性能瓶颈。想象一下，如果有 100 个程序要访问临界资源，那么协调者要处理 100*3=300 条消息。也就是说，协调者处理的消息数量会随着需要访问临界资源的程序数量线性增加。

* 另一方面，容易引发单点故障问题。协调者故障，会导致所有的程序均无法访问临界资源，导致整个系统不可用。

> 集中式算法具有**简单、易于实现**的特点，但**可用性、性能易受协调者影响**。在可靠性和性能有一定保障的情况下，比如中央服务器计算能力强、性能高、故障率低，或者中央服务器进行了主备，主故障后备可以立马升为主，且数据可恢复的情况下，集中式算法可以适用于比较广泛的应用场景。

>  优化： 可参照redis集群通信模式，通过hash key将大量的请求分散到不同的master,以处理大量请求,每个master由小集群主从节点来保障单点故障

##### 民主协商：分布式算法

当一个程序要访问临界资源时，**先向系统中的其他程序发送一条请求消息，在接收到所有程序返回的同意消息后**，才可以访问临界资源。其中，请求消息需要包含所请求的资源、请求者的 ID，以及发起请求的时间。在分布式领域中，我们称之为分布式算法，或者使用组播和逻辑时钟的算法。

总结来说，在大型系统中使用分布式算法，消息数量会随着需要访问临界资源的程序数量**呈指数级增加**，容易导致高昂的“沟通成本”。

分布式算法根据“先到先得”以及“投票全票通过”的机制，让每个程序**按时间顺序公平地访问资源，简单粗暴、易于实现**。

这个算法可用性很低

* 当系统内需要访问临界资源的程序增多时，容易产生“信令风暴”，也就是程序收到的请求完全超过了自己的处理能力，而导致自己正常的业务无法开展。
* 一旦某一程序发生故障，无法发送同意消息，那么其他程序均处在等待回复的状态中，使得整个系统处于停滞状态，导致整个系统不可用。所以，相对于集中式算法的协调者故障，分布式算法的可用性更低。(优化：如果检测到一个程序故障，则直接忽略这个程序，无需再等待它的同意消息)

分布式算法适合**节点数目少且变动不频繁的系统**，且由于**每个程序均需通信交互，因此适合 P2P 结构的系统**。比如，运行在局域网中的分布式文件系统，具有 P2P 结构的系统等。

> 优化： 分布式算法**可在集群中过半数同意就识为其同意**，降低通信数，如**分布式选举场景**

##### 轮值 CEO：令牌环算法

所有程序构成一个环结构，令牌按照顺时针（或逆时针）方向在程序之间传递，收到令牌的程序有权访问临界资源，访问完成后将令牌传送到下一个程序；若该程序不需要访问临界资源，则直接把令牌传送给下一个程序。

在分布式领域，这个算法叫作令牌环算法，也可以叫作基于环的算法。为了便于理解与记忆，你完全可以把这个方法形象地理解为轮值 CEO 法。

因为在使用临界资源前，不需要像分布式算法那样挨个征求其他程序的意见了，所以相对而言，在令牌环算法里单个程序具有更高的通信效率。同时，在一个周期内，每个程序都能访问到临界资源，因此令牌环算法的公平性很好。不管环中的程序是否想要访问资源，都需要接收并传递令牌，所以也会带来一些无效通信。设系统中有 100 个程序，那么程序 1 访问完资源后，即使其它 99 个程序不需要访问，也必须要等令牌在其他 99 个程序传递完后，才能重新访问资源，这就降低了系统的实时性。

令牌环算法非常适合通信模式为令牌环方式的分布式系统，例如移动自组织网络系统。一个典型的应用场景就是无人机通信。

令牌环算法的公平性高，在改进单点故障后，稳定性也很高，适用于系统规模较小，并且系统中每个程序使用临界资源的频率高且使用时间比较短的场景。

> 优化：可根据参与者使用频率列出权重，结合平滑加权轮询算法选出下一个参与者

> 由于大规模系统的复杂性，我们很自然地想到要用一个相对复杂的互斥算法。时下有一个很流行的互斥算法，**两层结构的分布式令牌环算法**，把整个广域网系统中的节点组织成两层结构，可以用于节点数量较多的系统，或者是广域网系统。
>
> 广域网由多个局域网组成，因此在该算法中，**局域网是较低的层次，广域网是较高的层次**。每个局域网中**包含若干个局部进程和一个协调进程**。局部进程在逻辑上组成一个环形结构，在每个环形结构上有一个局部令牌 T 在局部进程间传递。**局域网与局域网之间通过各自的协调进程进行通信**，这些协调进程同样组成一个环结构，这个环就是广域网中的全局环。在这个全局环上，有一个全局令牌在多个协调进程间传递。

![](https://static001.geekbang.org/resource/image/42/c6/4210e133d9d94ea22917db55458c11c6.png)



> 传统单机上的互斥方法，为什么不能用于分布式环境呢？
>
> 传统单机上的互斥只能针对单台机器上的程序相互间通信，而分布式环境往往是多台服务器上的程序相互通信

#### 分布式选举

集群一般是由两个或两个以上的服务器组建而成，每个服务器都是一个节点。分布式中协同管理的界面主节点，而选“领导”的过程在分布式领域中叫作分布式选举。

##### 长者为大：Bully 算法

选取 ID 最大的节点作为主节点。节点的角色有两种：**普通节点和主节点**。初始化时，所有节点都是平等的，都是普通节点，并且都有成为主的权利。但是，当选主成功后，**有且仅有一个节点成为主节点**，其他所有节点都是普通节点。当且仅**当主节点故障或与其他节点失去联系后，才会重新选主**。

Bully 算法在选举过程中，需要用到以下 3 种消息：

* Election 消息，用于发起选举；

* Alive 消息，对 Election 消息的应答；

* Victory 消息，竞选成功的主节点向其他节点发送的宣誓主权的消息。

> Bully 算法选举的原则是“长者为大”，意味着它的假设条件是，**集群中每个节点均知道其他节点的 ID**。
>
> 在此前提下，其具体的选举过程是：
>
> 集群中每个节点**判断自己的 ID 是否为当前活着的节点中 ID 最大的**，如果是，则直接向其他节点发送 Victory 消息，宣誓自己的主权；如果自己不是当前活着的节点中 ID 最大的，**则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复**；若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，**并向其他节点发送 Victory 消息，宣誓自己成为主节点**；若接收到来自比自己 ID 大的节点的 Alive 消息，则等待其他节点发送 Victory 消息；若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大，重新选举。

![](https://raw.githubusercontent.com/calmpeng/photo/main/img/91385c487255ba0179d8e9538ed8f154.png)

Bully 算法的选择特别霸道和简单，谁活着且谁的 ID 最大谁就是主节点，其他节点必须无条件服从。这种算法的优点是，**选举速度快、算法复杂度低、简单易实现**。

需要每个节点**有全局的节点信息，因此额外信息存储较多**；其次，**任意一个比当前主节点 ID 大的新节点或节点故障后恢复加入集群的时候，都可能会触发重新选举，成为新的主节点**，如果该节点频繁退出、加入集群，就会导致频繁切主。

##### 民主投票：Raft 算法

Raft 算法是典型的多数派投票选举算法，核心思想是“少数服从多数”。也就是说，Raft 算法中，获得投票最多的节点成为主。

集群节点的角色有 3 种：

* Leader，即主节点，同一时刻只有一个 Leader，负责协调和管理其他节点；
* Candidate，即候选者，每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 Leader；
* Follower，Leader 的跟随者，不可以发起选举。

> Raft 选举的流程，可以分为以下几步：
>
> 初始化时，所有节点均为 Follower 状态。开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。其**他节点根据接收到的选举请求的先后顺序，回复是否同意成为主**。这里需要注意的是，**在每一轮选举中，一个节点只能投出一张票**。若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，其他节点的状态则由 Candidate 降为 Follower。
>
> Leader 节点与 Follower 节点之间会**定期发送心跳包**，以检测主节点是否活着。**当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时**，Leader 节点的状态由 Leader 降级为 Follower，进入新一轮选主。

![](https://raw.githubusercontent.com/calmpeng/photo/main/img/fc0f00a3b7c9290bc91cb4d8721dc6b8.png)

**每一轮选举，每个节点只能投一次票**。对应到 Raft 算法中，选主是周期进行的，包括选主和任值两个时间段，选主阶段对应投票阶段，任值阶段对应节点成为主之后的任期。但也有例外的时候，如果主节点故障，会立马发起选举，重新选出一个主节点。

> Kubernetes 的选主采用的是开源的 etcd 组件。而，etcd 的集群管理器 etcds，是一个高可用、强一致性的服务发现存储仓库，就是采用了 Raft 算法来实现选主和一致性的。

Raft 算法具有**选举速度快、算法复杂度低、易于实现**的优点；缺点是，它要求系统内每个节点都可以相互通信，且需要获得过半的投票数才能选主成功，因此**通信量大**该算法选举稳定性比 Bully 算法好，这是因为当有新节点加入或节点故障恢复后，**会触发选主，但不一定会真正切主**，除非新节点或故障后恢复的节点获得投票数过半，才会导致切主。

#### 具有优先级的民主投票：ZAB 算法

ZAB（ZooKeeper Atomic Broadcast）选举算法是为 ZooKeeper 实现分布式协调功能而设计的。

相较于 Raft 算法的投票机制，ZAB 算法增加了**通过节点 ID 和数据 ID 作为参考**进行选主，节点 ID 和数据 ID 越大，表示数据越新，优先成为主。相比较于 Raft 算法，ZAB 算法尽可能保证数据的最新性。所以，ZAB 算法可以说是对 Raft 算法的改进。

集群中每个节点拥有 3 种角色：

* Leader，主节点；
* Follower，跟随者节点；
* Observer，观察者，无投票权。

集群中的节点拥有 4 个状态：

* Looking 状态，即选举状态。当节点处于该状态时，它会认为当前集群中没有 Leader，因此自己进入选举状态。

* Leading 状态，即领导者状态，表示已经选出主，且当前节点为 Leader。

* Following 状态，即跟随者状态，集群中已经选出主后，其他非主节点状态更新为 Following，表示对 Leader 的追随。

* Observing 状态，即观察者状态，表示当前节点为 Observer，持观望态度，没有投票权和选举权。

每个节点都有一个唯一的三元组 (server_id, server_zxID, epoch)，其中 **server_id 表示本节点的唯一 ID**；**server_zxID 表示本节点存放的数据 ID，数据 ID 越大表示数据越新，选举权重越大**；epoch 表示当前选取轮数，一般用逻辑时钟表示。

ZAB 选举算法的核心是“少数服从多数，ID 大的节点优先成为主”，因此选举过程中通过 (vote_id, vote_zxID) 来表明投票给哪个节点，其中 vote_id 表示被投票节点的 ID，vote_zxID 表示被投票节点的服务器 zxID。ZAB 算法选主的原则是：server_zxID 最大者成为 Leader；若 server_zxID 相同，则 server_id 最大者成为 Leader。

ZAB 算法性能高，对系统无特殊要求，**采用广播方式发送信息**，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息，**容易出现广播风暴**；且除了投票，还增加了对比节点 ID 和数据 ID，**这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长。**但该算法选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主。

![](https://raw.githubusercontent.com/calmpeng/photo/main/img/e411f24b0b03991ad761134dfc3dff7e.jpg)

> 多数派选主算法的核心是少数服从多数，获得投票多的节点胜出。想象一下，如果现在采用偶数节点集群，当两个节点均获得一半投票时，到底应该选谁为主呢？答案是，在这种情况下，无法选出主，必须重新投票选举。但即使重新投票选举，两个节点拥有相同投票数的概率也会很大。因此，多数派选主算法通常采用奇数节点。这，也是大家通常看到 ZooKeeper、 etcd、Kubernetes 等开源软件选主均采用奇数节点的一个关键原因。

![](https://raw.githubusercontent.com/calmpeng/photo/main/img/04dfd1e4b8a1558fcbfa1bb8a9b077bd.png)

#### 分布式共识

选主过程就是一个分布式共识问题，因为每个节点在选出主节点之前都可以认为自己会成为主节点，也就是说集群节点“存异”；而通过选举的过程选出主节点，让所有的节点都认可该主节点，这叫“求同”。由此可见，**分布式共识的本质就是“存异求同”。**

**从本质上看，分布式选举问题，其实就是传统的分布式共识方法，主要是基于多数投票策略实现的。**

**分布式共识就是在多个节点均可独自操作或记录的情况下，使得所有节点针对某个状态达成一致的过程。**通过共识机制，我们可以使得分布式系统中的多个节点的数据达成一致。

**分布式共识包括两个关键点，获得记账权和所有节点或服务器达成一致**。

##### 分布式共识方法

* PoW（Proof-of-Work，工作量证明）

  **PoW 算法**，是以每个节点或服务器的计算能力（即“算力”）来竞争记账权的机制，因此是一种**使用工作量证明机制的共识算法**。也就是说，谁的计算力强、工作能力强，谁获得记账权的可能性就越大。

  其实竞争的就是挖矿设备，看谁的挖矿设备的 CPU、GPU 等更厉害，缺点就是费电、污染环境。

* PoS（Proof-of-Stake，权益证明）

  PoS 算法，由系统权益代替算力来决定区块记账权，拥有的权益越大，获得记账权的概率就越大。这种方法的优点是节能，不需要挖矿了，但缺点是容易形成垄断。

* DPoS（Delegated Proof of Stake，委托权益证明）

  DPoS 算法，是一种委托权益证明算法。持有币的人可以通过投票选举出一些节点，来作为代表去记账，类似于全国人民代表大会制度。

![image-20220329234949365](https://raw.githubusercontent.com/calmpeng/photo/main/img/image-20220329234949365.png)

![image-20220329235011060](https://raw.githubusercontent.com/calmpeng/photo/main/img/image-20220329235011060.png)

> **一致性**是指，分布式系统中的多个节点之间，给定一系列的操作，在约定协议的保障下，对外界呈现的数据或状态是一致的。
>
> **共识**是指，分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。
>
> **一致性强调的是结果，共识强调的是达成一致的过程**，共识算法是保障系统满足不同程度一致性的核心技术。







